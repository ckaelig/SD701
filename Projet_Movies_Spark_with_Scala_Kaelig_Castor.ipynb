{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> PROJECT : \"SPARK with SCALA\" </h1> </center>\n",
    "\n",
    "<center> <h2> Recommendation System on Movies </h2> </center>\n",
    "\n",
    "<center> <h3> 2019/12/26 </h3> </center>\n",
    "\n",
    "<center> <h3> Kaelig Castor </h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='text-align: justify;'> <b> \n",
    "This part of the project is coded in scala. <br> \n",
    "<br>\n",
    "The recommendation system relies on a classical dataset grouping blockbuster ratings. The dataset is already cleaned and labelled. <br> \n",
    "<br>\n",
    "Please change the following data path to access the files (movies.csv and ratings.csv) and run the notebook.</p> </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://172.27.96.1:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1577571979192)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "path_to_data: String = /mnt/d/04_EXPLORATION_GRANDS_VOLUMES_DONNEES/ml-latest-small/\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path_to_data : String = \"/mnt/d/04_EXPLORATION_GRANDS_VOLUMES_DONNEES/ml-latest-small/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we define the configuration and initialize the SPARK session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.types.IntegerType\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@58e37bd4\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@77086eff\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.types.IntegerType\n",
    "val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\"\n",
    "    ))\n",
    "val spark = SparkSession\n",
    "      .builder\n",
    "      .config(conf)\n",
    "      .appName(\"Projet Spark\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second, we read the data files, create the dataframes, and check everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 100836\n",
      "Nombre de colonnes : 4\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n",
      "Nombre de lignes : 9742\n",
      "Nombre de colonnes : 3\n",
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@1caf54ee\n",
       "import org.apache.spark.sql.SQLImplicits\n",
       "dfR: org.apache.spark.sql.DataFrame = [userId: int, movieId: int ... 2 more fields]\n",
       "dfRatings: org.apache.spark.sql.DataFrame = [userId: int, movieId: int ... 2 more fields]\n",
       "dfM: org.apache.spark.sql.DataFrame = [movieId: int, title: string ... 1 more field]\n",
       "dfMovies: org.apache.spark.sql.DataFrame = [movieId: int, title: string ... 1 more field]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "import org.apache.spark.sql.SQLImplicits\n",
    "val dfR: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true)         \n",
    "      .option(\"inferSchema\", \"true\")  \n",
    "      .csv(path_to_data + \"ratings.csv\")\n",
    "println(s\"Nombre de lignes : ${dfR.count}\")\n",
    "println(s\"Nombre de colonnes : ${dfR.columns.length}\")\n",
    "dfR.show() // Display the ratings  \n",
    "dfR.printSchema()\n",
    "val dfRatings: DataFrame = dfR\n",
    "      .withColumn(\"userId\" , $\"userId\".cast(\"Int\"))\n",
    "      .withColumn(\"movieId\", $\"movieId\".cast(\"Int\"))\n",
    "      .withColumn(\"rating\", $\"rating\".cast(\"Int\"))\n",
    "      .withColumn(\"timestamp\", $\"timestamp\".cast(\"Int\"))\n",
    "dfRatings.printSchema()\n",
    "\n",
    "val dfM: DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true)         \n",
    "      .option(\"inferSchema\", \"true\")  \n",
    "      .csv(path_to_data + \"movies.csv\")\n",
    "println(s\"Nombre de lignes : ${dfM.count}\")\n",
    "println(s\"Nombre de colonnes : ${dfM.columns.length}\")\n",
    "dfM.show() // Display the movies  \n",
    "dfM.printSchema()\n",
    "val dfMovies: DataFrame = dfM\n",
    "      .withColumn(\"movieId\", $\"movieId\".cast(\"Int\"))\n",
    "dfMovies.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Filtering (CF)\n",
    "\n",
    "<p style='text-align: justify;'> In the following we use one of the most classical collaborative filtering technique. Reminding that collaborative filtering is commonly used for recommender systems, <b>Collaborative Filtering (CF)</b> is a method of making <b>automatic predictions</b> about the interests of a user by learning its preferences (or taste) based on information of his engagements with a set of available items, along with other users’ engagements with the same set of items. In other words, CF assumes that, if a person A has the <b>same opinion as</b> person B on some set of issues X={x1,x2,…}, then A is more likely to have B's opinion on a new issue y than to have the opinion of any other person that doesn’t agree with A on X.</p> \n",
    "\n",
    "<p style='text-align: justify;'> So for example, while person A's favorite movies are {<i>American Hustle</i>, <i>Hunger Games</i> and <i>Delivery Man</i>}, person B really likes {<i>American Hustle</i>, <i>Hunger Games</i> and <i>Captain Philips</i>} and person C just loves {<i>12 Years a Slave</i>, <i>Reasonable Doubt</i> and <i>Die Hard II</i>}, following CF approach it should be safer to assume person B should also like <i>Delivery Man</i> and person A would have liked to watch <i>Captain Philips</i>, than to assume any of them would be interested in watching <i>12 Years a Slave</i>, <i>Reasonable Doubt</i> or <i>Die Hard II</i>.</p> \n",
    "\n",
    "<p style='text-align: justify;'> By discovering usage/engagement patterns among users and items in the data, <b>CF algorithms are able to infer users’ hidden preferences and to exploit those preferences to recommend them new potentially-good items.</b> CF algorithms are best known for their use on e-commerce web sites, where they serve as cornerstones for their recommendation-engines.</p> \n",
    "\n",
    "<p style='text-align: justify;'> First, we describe one of the most popular CF algorithm, known as <b>Matrix Factorization.</b> Then we will describe the <b>Alternating Least Squares minimization.</b></p> \n",
    "\n",
    "\n",
    "#### Matrix Factorization\n",
    "\n",
    "<p style='text-align: justify;'> One of the most popular algorithms to solve co-clustering problems (and specifically for collaborative recommender systems) is called Matrix Factorization (MF). In its simplest form, it assumes a matrix ${{R}\\in{R^{m \\times n}}}$ of ratings given by musers to nitems. Applying this technique on $R$ will end up factorizing $R$ into two matrices ${{U}\\in{R^{m \\times k}}}$ and ${{P}\\in{R^{n \\times k}}}$ such that $R \\approx U \\times P$ (their multiplication approximates $R$).</p> \n",
    "\n",
    "\n",
    "#### Alternating Least Squares minimization\n",
    "\n",
    "<p style='text-align: justify;'> Alternating minimization represents a widely applicable and empirically successful approach for finding low-rank matrices that best fit the given data. Looking again at MF’s cost function, it appears that we aim at learning two types of variables – those of $U$ and those of $P$, and the two types are tied in the multiplication of $U \\times {P^T}$. Recall that the actual cost function is the $\\sum ||R-{U}\\times{P^T}||_2$ = $\\mathop \\sum\\limits_{i,j}\\left({R_{i,j}-{u_i}\\times{p_j}}\\right)$ plus regularization term. The fact that both $U$’s and $V$’s values are unknown variables makes this cost function non-convex.</p> \n",
    "\n",
    "<p style='text-align: justify;'> But another interesting fact lies in this term – if we fix $P$ and optimize for $U$ alone, the problem is simply reduced to the problem of linear regression. Recall that in linear regression, we solve for $\\theta$ by minimizing the squared error $||y - X\\theta||_2$ given $X$ and $y$. The solution is ultimately given by the Ordinary Least Squares (OLS) estimator ${\\hat{\\theta} = \\left({{X^T}X}\\right)^{ - 1}}{X^T}y$.</p> \n",
    "\n",
    "<p style='text-align: justify;'> Alternating least squares does just that. It is <b>a two-step iterative optimization process.</b> In every iteration, it first fixes $P$ and solves for $U$, and following that it fixes $U$ and solves for $P$. Since OLS solution is unique and guarantees a minimal MSE, in each step the cost function can either decrease or stay unchanged, but never increase. Alternating between the two steps guarantees reduction of the cost function, until convergence. Similarly to gradient descent optimization, it is guaranteed to converge only to a local minima, and is ultimately depends on initial values for $U$ or $P$.</p> \n",
    "\n",
    "<p style='text-align: justify;'> Since the actual cost function includes a regularization term, it is slightly longer. According to the two-step process, the cost function can be broken down into two cost functions:</p> \n",
    "\n",
    "$\\forall{u_i}: J\\left({u_i}\\right) = ||R_i - {u_i}\\times{P^T}||_2 + \\lambda \\cdot ||u_i||_2$\n",
    "\n",
    "$\\forall{p_j}: J\\left({p_j}\\right) = ||R_i - U\\times{p_j^T}||_2 + \\lambda \\cdot ||p_j||_2$\n",
    "\n",
    "And the matching solutions for ${u_i}$ and ${p_j}$ are:\n",
    "\n",
    "${u_i} = {\\left( {{P^T} \\times P + \\lambda I} \\right)^{ - 1}} \\times {P^T} \\times {R_i}$\n",
    "\n",
    "${p_j} = {\\left( {{U^T} \\times U + \\lambda I} \\right)^{ - 1}} \\times {U^T} \\times {R_j}$\n",
    "\n",
    "<p style='text-align: justify;'> And since each ${u_i}$ doesn’t depend on other ${u_{j \\ne i}}$ vectors, each step can potentially be introduced to massive parallelization.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 3.1013445961764288\n",
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|    50|[[4993, 0.9183656...|\n",
      "|   280|[[4306, 0.8846774...|\n",
      "|   300|[[318, 0.6642463]...|\n",
      "|   400|[[318, 0.8859384]...|\n",
      "|   161|[[595, 0.37635446...|\n",
      "|   411|[[590, 1.0840641]...|\n",
      "|   471|[[318, 0.6441616]...|\n",
      "|   591|[[2571, 0.5389139...|\n",
      "|   172|[[593, 0.27602068...|\n",
      "|   232|[[33679, 1.301014...|\n",
      "|   282|[[2571, 1.2251546...|\n",
      "|   452|[[1240, 1.1373057...|\n",
      "|   482|[[780, 0.71582305...|\n",
      "|    73|[[68954, 1.060413...|\n",
      "|   233|[[318, 1.1297597]...|\n",
      "|   263|[[4306, 1.0056376...|\n",
      "|   343|[[318, 0.7205763]...|\n",
      "|   363|[[318, 0.5212913]...|\n",
      "|   403|[[2571, 0.2521280...|\n",
      "|   413|[[318, 0.69170713...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.recommendation.ALS\n",
       "defined class Rating\n",
       "parseRating: (str: String)Rating\n",
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userId: int, movieId: int ... 2 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userId: int, movieId: int ... 2 more fields]\n",
       "als: org.apache.spark.ml.recommendation.ALS = als_44d84b341671\n",
       "model: org.apache.spark.ml.recommendation.ALSModel = als_44d84b341671\n",
       "predictions: org.apache.spark.sql.DataFrame = [userId: int, movieId: int ... 3 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_f825b2ea5167\n",
       "rmse: Double = 3.1013445961764288\n",
       "userRecs: org.apache.spark.sql.DataFrame = [userId: int, recommendations:..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "\n",
    "case class Rating(userId: Int, movieId: Int, rating: Float, timestamp: Long)\n",
    "def parseRating(str: String): Rating = {\n",
    "              val fields = str.split(\"::\")\n",
    "              assert(fields.size == 4)\n",
    "              Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat, fields(3).toLong)\n",
    "}\n",
    "\n",
    "// SPLITTING THE DATASET INTO TRAINING (80%) and TEST (20%) SUBSETS\n",
    "val Array(training, test) = dfRatings.randomSplit(Array(0.8, 0.2))\n",
    "\n",
    "// Build the recommendation model using ALS (Alternating Least Squares) matrix factorization on the training data\n",
    "val als = new ALS()\n",
    "  .setMaxIter(5)\n",
    "  .setRegParam(0.01)\n",
    "  .setImplicitPrefs(true)\n",
    "  .setUserCol(\"userId\")\n",
    "  .setItemCol(\"movieId\")\n",
    "  .setRatingCol(\"rating\")\n",
    "\n",
    "// Training the ALS model\n",
    "val model = als.fit(training) \n",
    "\n",
    "// Evaluate the model by computing the RMSE on the test data\n",
    "// Note we set cold start strategy to 'drop' to ensure we don't get NaN evaluation metrics\n",
    "model.setColdStartStrategy(\"drop\")\n",
    "val predictions = model.transform(test)  \n",
    "\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setMetricName(\"rmse\")\n",
    "  .setLabelCol(\"rating\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root-mean-square error = $rmse\")\n",
    "\n",
    "// Generate top 10 movie recommendations for each user\n",
    "val userRecs = model.recommendForAllUsers(10)\n",
    "userRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|     14|[[474, 0.89270496...|\n",
      "|     18|[[599, 1.0080756]...|\n",
      "|     25|[[387, 1.2729722]...|\n",
      "|     38|[[448, 0.5665329]...|\n",
      "|     46|[[474, 0.94490904...|\n",
      "|     50|[[28, 1.2722144],...|\n",
      "|     73|[[599, 0.83765906...|\n",
      "|     97|[[599, 0.81554574...|\n",
      "|    161|[[6, 1.3349816], ...|\n",
      "|    172|[[599, 1.4691492]...|\n",
      "|    186|[[6, 0.98019737],...|\n",
      "|    225|[[6, 1.275411], [...|\n",
      "|    232|[[606, 0.8815048]...|\n",
      "|    233|[[599, 0.86460394...|\n",
      "|    248|[[599, 0.73354214...|\n",
      "|    254|[[6, 0.17176375],...|\n",
      "|    257|[[414, 1.0522056]...|\n",
      "|    263|[[606, 0.52587324...|\n",
      "|    280|[[474, 0.63333803...|\n",
      "|    282|[[6, 1.14901], [1...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movieRecs: org.apache.spark.sql.DataFrame = [movieId: int, recommendations: array<struct<userId:int,rating:float>>]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generate top 10 user recommendations for each movie\n",
    "val movieRecs = model.recommendForAllItems(10)\n",
    "movieRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|    50|[[4993, 0.9183656...|\n",
      "|   161|[[595, 0.37635446...|\n",
      "|   172|[[593, 0.27602068...|\n",
      "|    73|[[68954, 1.060413...|\n",
      "|    14|[[296, 0.7601203]...|\n",
      "|    25|[[318, 0.5504094]...|\n",
      "|    46|[[296, 0.8334836]...|\n",
      "|    97|[[2571, 0.5067856...|\n",
      "|    18|[[318, 1.4194739]...|\n",
      "|    38|[[356, 0.96146435...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "users: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [userId: int]\n",
       "userSubsetRecs: org.apache.spark.sql.DataFrame = [userId: int, recommendations: array<struct<movieId:int,rating:float>>]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generate top 10 movie recommendations for a specified set of users\n",
    "val users = dfRatings.select(als.getUserCol).distinct().limit(10)\n",
    "val userSubsetRecs = model.recommendForUserSubset(users, 10)\n",
    "userSubsetRecs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|movieId|     recommendations|\n",
      "+-------+--------------------+\n",
      "|     50|[[28, 1.2722144],...|\n",
      "|   1030|[[474, 1.0218881]...|\n",
      "|   1092|[[414, 1.0558394]...|\n",
      "|   2093|[[414, 0.7096334]...|\n",
      "|   1214|[[469, 1.3113378]...|\n",
      "|   2414|[[414, 0.8183437]...|\n",
      "|   1256|[[448, 0.92395514...|\n",
      "|   2366|[[448, 0.9029837]...|\n",
      "|    919|[[474, 1.4919729]...|\n",
      "|   1029|[[448, 0.9125159]...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "movies: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [movieId: int]\n",
       "movieSubSetRecs: org.apache.spark.sql.DataFrame = [movieId: int, recommendations: array<struct<userId:int,rating:float>>]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Generate top 10 user recommendations for a specified set of movies\n",
    "val movies = dfRatings.select(als.getItemCol).distinct().limit(10)\n",
    "val movieSubSetRecs = model.recommendForItemSubset(movies, 10)\n",
    "movieSubSetRecs.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
